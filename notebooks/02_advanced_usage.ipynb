{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR Advanced Usage\n",
    "\n",
    "This notebook demonstrates advanced RAPTOR features:\n",
    "1. LiteLLM integration (100+ providers)\n",
    "2. HuggingFace embeddings\n",
    "3. Cross-encoder reranking\n",
    "4. Custom text splitting\n",
    "5. Tree inspection\n",
    "6. Framework integrations (LangChain + LlamaIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\n```bash\npip install raptor-rag[all]\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set your API key (or use LiteLLM's environment variables)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LiteLLM Integration\n",
    "\n",
    "Use any LLM provider with a single model string. LiteLLM supports OpenAI, Anthropic, Cohere, Mistral, local models, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import (\n",
    "    RetrievalAugmentation,\n",
    "    RetrievalAugmentationConfig,\n",
    "    LiteLLMSummarizationModel,\n",
    "    LiteLLMQAModel,\n",
    ")\n",
    "\n",
    "# Use Anthropic for summarization, OpenAI for QA\n",
    "config = RetrievalAugmentationConfig(\n",
    "    summarization_model=LiteLLMSummarizationModel(\n",
    "        model=\"anthropic/claude-3-haiku-20240307\",\n",
    "        system_prompt=\"You are an expert at creating concise summaries.\",\n",
    "    ),\n",
    "    qa_model=LiteLLMQAModel(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        user_prompt_template=\"Context: {context}\\n\\nQuestion: {question}\\nProvide a detailed answer:\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ra = RetrievalAugmentation(config=config)\n",
    "print(\"LiteLLM config created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HuggingFace Embeddings\n",
    "\n",
    "Use any HuggingFace sentence-transformer model instead of OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import HuggingFaceEmbeddingModel, RetrievalAugmentationConfig\n",
    "\n",
    "# BGE-small is fast and high quality\n",
    "embedding_model = HuggingFaceEmbeddingModel(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    device=None,  # Auto-detect GPU/CPU\n",
    ")\n",
    "\n",
    "# Test it\n",
    "emb = embedding_model.create_embedding(\"Hello world\")\n",
    "print(f\"Embedding dimension: {len(emb)}\")\n",
    "print(f\"First 5 values: {emb[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Encoder Reranking\n",
    "\n",
    "Add a reranking stage after initial retrieval for higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import CrossEncoderReRanker, RetrievalAugmentationConfig\n",
    "\n",
    "# Cross-encoder reranker\n",
    "reranker = CrossEncoderReRanker(\n",
    "    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    device=None,\n",
    ")\n",
    "\n",
    "config = RetrievalAugmentationConfig(\n",
    "    tr_reranker=reranker,\n",
    "    # Retrieve more candidates, then rerank to top results\n",
    "    tr_top_k=20,\n",
    ")\n",
    "\n",
    "print(\"Reranker config created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Text Splitting\n",
    "\n",
    "Implement your own text splitting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import BaseTextSplitter, DefaultTextSplitter\n",
    "\n",
    "# Default splitter with overlap\n",
    "overlapping_splitter = DefaultTextSplitter(overlap=2)\n",
    "\n",
    "# Custom paragraph-based splitter\n",
    "class ParagraphSplitter(BaseTextSplitter):\n",
    "    def split_text(self, text, tokenizer, max_tokens):\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "        # Merge small paragraphs, split large ones\n",
    "        chunks = []\n",
    "        current = []\n",
    "        current_tokens = 0\n",
    "        for para in paragraphs:\n",
    "            para_tokens = len(tokenizer.encode(para))\n",
    "            if current_tokens + para_tokens > max_tokens and current:\n",
    "                chunks.append(\"\\n\\n\".join(current))\n",
    "                current = []\n",
    "                current_tokens = 0\n",
    "            current.append(para)\n",
    "            current_tokens += para_tokens\n",
    "        if current:\n",
    "            chunks.append(\"\\n\\n\".join(current))\n",
    "        return chunks\n",
    "\n",
    "print(\"Custom splitters created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tree Inspection\n",
    "\n",
    "Examine the tree structure after building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nfrom pathlib import Path\n\n# Load the demo tree (run 01_basic_usage.ipynb first to generate it)\ntree_path = Path(\"../data/cinderella\")\nif tree_path.exists():\n    with open(tree_path, \"rb\") as f:\n        tree = pickle.load(f)\n    \n    print(f\"Number of layers: {tree.num_layers}\")\n    print(f\"Total nodes: {len(tree.all_nodes)}\")\n    print(f\"Leaf nodes: {len(tree.leaf_nodes)}\")\n    print(f\"Root nodes: {len(tree.root_nodes)}\")\n    \n    print(\"\\nNodes per layer:\")\n    for layer, nodes in tree.layer_to_nodes.items():\n        print(f\"  Layer {layer}: {len(nodes)} nodes\")\n    \n    # Inspect a leaf node\n    leaf = list(tree.leaf_nodes.values())[0]\n    print(f\"\\nSample leaf node (index {leaf.index}):\")\n    print(f\"  Text: {leaf.text[:100]}...\")\n    print(f\"  Children: {leaf.children}\")\n    print(f\"  Embedding models: {list(leaf.embeddings.keys())}\")\nelse:\n    print(\"Tree not found. Run 01_basic_usage.ipynb first to build it.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Framework Integrations\n",
    "\n",
    "### LangChain\n",
    "\n",
    "```bash\n",
    "pip install langchain-raptor-rag\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain integration example\n",
    "# from raptor import RetrievalAugmentation\n",
    "# from langchain_raptor_rag import RaptorRetriever\n",
    "#\n",
    "# ra = RetrievalAugmentation()\n",
    "# ra.add_documents(text)\n",
    "# retriever = RaptorRetriever(ra=ra, top_k=10, max_tokens=3500)\n",
    "# docs = retriever.invoke(\"What happened to Cinderella?\")\n",
    "# for doc in docs:\n",
    "#     print(f\"Layer {doc.metadata['layer_number']}: {doc.page_content[:80]}...\")\n",
    "\n",
    "print(\"See langchain-raptor-rag package for full integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LlamaIndex\n",
    "\n",
    "```bash\n",
    "pip install llama-index-raptor-rag\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex integration example\n",
    "# from raptor import RetrievalAugmentation\n",
    "# from llama_index_raptor_rag import RaptorRetriever\n",
    "# from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "#\n",
    "# ra = RetrievalAugmentation()\n",
    "# ra.add_documents(text)\n",
    "# retriever = RaptorRetriever(ra=ra)\n",
    "# query_engine = RetrieverQueryEngine.from_args(retriever)\n",
    "# response = query_engine.query(\"What happened to Cinderella?\")\n",
    "# print(response)\n",
    "\n",
    "print(\"See llama-index-raptor-rag package for full integration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}